import os
from dotenv import load_dotenv
from pathlib import Path
import pickle
import gradio as gr

from src.vector_store import load_vector_store, retrieve_chunks
from src.llm_backend import llm, template, StructuredAnswer

# Load env
load_dotenv()

# Load vector store
vector_store = load_vector_store()
if vector_store is None:
    raise ValueError("‚ùå Vector store ch∆∞a ƒë∆∞·ª£c t·∫°o. H√£y ch·∫°y vector_store.py tr∆∞·ªõc.")

# H√†m chat
def chat_fn(user_input, history):
    """
    user_input: c√¢u h·ªèi ng∆∞·ªùi d√πng
    history: danh s√°ch history [(user_msg, bot_msg), ...]
    """
    if not user_input.strip():
        return history  # n·∫øu tr·ªëng th√¨ kh√¥ng tr·∫£ l·ªùi

    # 1. L·∫•y top chunk t·ª´ vector store
    top_chunks = retrieve_chunks(vector_store, user_input, k=5)

    # 2. T·∫°o context string cho LLM
    context_text = "\n\n".join(
        f"T√†i li·ªáu: {chunk.metadata.get('source', 'Kh√¥ng r√µ')}\n{chunk.page_content}"
        for chunk in top_chunks
    )

    # 3. T·∫°o prompt
    from langchain_core.prompts import ChatPromptTemplate
    prompt = ChatPromptTemplate.from_template(template)
    messages = prompt.format_messages(question=user_input, context=context_text)

    # 4. LLM tr·∫£ StructuredAnswer
    structured_llm = llm.with_structured_output(StructuredAnswer)
    structured_response: StructuredAnswer = structured_llm.invoke(messages)

    # 5. Convert th√†nh string ƒë·ªÉ Gradio hi·ªÉu
    bot_message = f"{structured_response.answer}\n(Ngu·ªìn: {structured_response.source})"

    # 6. Update history
    history = history or []
    history.append((user_input, bot_message))

    return history

# Launch Gradio chat interface
with gr.Blocks() as demo:
    gr.Markdown("## üö¶ Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Nh·∫≠p c√¢u h·ªèi...")
    clear = gr.Button("Clear")

    msg.submit(chat_fn, inputs=[msg, chatbot], outputs=[chatbot])
    clear.click(lambda: [], None, chatbot)

demo.launch(share=True)
